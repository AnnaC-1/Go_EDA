{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a CSV File for data to be put into "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "output_csv_file = \"/Users/annacapels/Desktop/DS Projects/GO EDA/output1.csv\"\n",
    "\n",
    "# Define the headers for the CSV file\n",
    "headers = [\"Board Size\", \"Format Version\", \"White Player's Name\", \"Black Player's Name\", \"Date of Game\", \"Komi\", \"Result of Game\", \"Moves of Game\", \"Move Color\", \"Move Type\" ]\n",
    "\n",
    "# Open the output CSV file\n",
    "with open(output_csv_file, \"w\", newline=\"\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(headers)\n",
    "\n",
    "print(\"CSV file created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking every SGF file path and putting them into list called sgf_directories and then put them into a dictionary titled merged_dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# List of directories containing SGF files\n",
    "sgf_directories = [\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/0196-1989-Database-July/0196-1699\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/0196-1989-Database-July/1700-99\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/0196-1989-Database-July/1800-49\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/0196-1989-Database-July/1850-99\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/0196-1989-Database-July/1900-09\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/0196-1989-Database-July/1910-19\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/0196-1989-Database-July/1920-29\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/0196-1989-Database-July/1930-39\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/0196-1989-Database-July/1940-49\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/0196-1989-Database-July/1950-59\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/0196-1989-Database-July/1960-69\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/0196-1989-Database-July/1970-75\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/0196-1989-Database-July/1976-79\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/0196-1989-Database-July/1980\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/0196-1989-Database-July/1981\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/0196-1989-Database-July/1982\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/0196-1989-Database-July/1983\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/0196-1989-Database-July/1984\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/0196-1989-Database-July/1985\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/0196-1989-Database-July/1986\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/0196-1989-Database-July/1987\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/0196-1989-Database-July/1988\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/0196-1989-Database-July/1989\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/1990-2009-Database-July/1990\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/1990-2009-Database-July/1991\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/1990-2009-Database-July/1992\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/1990-2009-Database-July/1993\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/1990-2009-Database-July/1994\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/1990-2009-Database-July/1995\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/1990-2009-Database-July/1996\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/1990-2009-Database-July/1997\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/1990-2009-Database-July/1998\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/1990-2009-Database-July/1999\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/1990-2009-Database-July/2000\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/1990-2009-Database-July/2001\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/1990-2009-Database-July/2002\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/1990-2009-Database-July/2003\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/1990-2009-Database-July/2004\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/1990-2009-Database-July/2005\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/1990-2009-Database-July/2006\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/1990-2009-Database-July/2007\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/1990-2009-Database-July/2008\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/1990-2009-Database-July/2009\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/2010-2022-Database-July/2010\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/2010-2022-Database-July/2011\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/2010-2022-Database-July/2012\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/2010-2022-Database-July/2013\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/2010-2022-Database-July/2014\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/2010-2022-Database-July/2015\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/2010-2022-Database-July/2016\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/2010-2022-Database-July/2017\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/2010-2022-Database-July/2018\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/2010-2022-Database-July/2019\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/2010-2022-Database-July/2020\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/2010-2022-Database-July/2021\",\n",
    "\"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/SGF/19x19 Boards/2010-2022-Database-July/2022\"]\n",
    "\n",
    "# Destination directory for merged SGF files\n",
    "merged_directory = \"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/Merged-SGF-Files\"\n",
    "\n",
    "# Create the destination directory if it doesn't exist\n",
    "if not os.path.exists(merged_directory):\n",
    "    os.makedirs(merged_directory)\n",
    "\n",
    "# Iterate through the source directories and copy the SGF files to the destination directory\n",
    "for sgf_directory in sgf_directories:\n",
    "    for filename in os.listdir(sgf_directory):\n",
    "        if filename.endswith(\".sgf\"):\n",
    "            src = os.path.join(sgf_directory, filename)\n",
    "            dst = os.path.join(merged_directory, filename)\n",
    "            shutil.copyfile(src, dst)\n",
    "\n",
    "\n",
    "#This script will copy all of the SGF files from the directories listed in `sgf_directories` to the `merged_directory` directory. \n",
    "#Run Time: ~55 seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import csv\n",
    "import sgf\n",
    "from sgf import parse, ParseException\n",
    "\n",
    "merged_directory = \"/Users/annacapels/Desktop/DS Projects/GO EDA/GoGoD-Data/Merged-SGF-Files\"\n",
    "output_csv_file = \"/Users/annacapels/Desktop/DS Projects/GO EDA/output1.csv\"\n",
    "\n",
    "# Define the headers for the CSV file\n",
    "headers = [\"Board Size\", \"Format Version\", \"White Player's Name\", \"Black Player's Name\", \"Date of Game\", \"Komi\",\n",
    "           \"Result of Game\", \"Moves of Game\", \"Move Color\", \"Move Type\"]\n",
    "\n",
    "# Open the output CSV file\n",
    "with open(output_csv_file, \"w\", newline=\"\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(headers)\n",
    "\n",
    "    # Iterate through the SGF files in the directory\n",
    "    for filename in os.listdir(merged_directory):\n",
    "        sgf_path = os.path.join(merged_directory, filename)\n",
    "\n",
    "        # Validate the SGF file\n",
    "        try:\n",
    "            with open(sgf_path, \"r\", encoding='ISO-8859-1') as f:\n",
    "                contents = f.read()\n",
    "                sgf.parse(contents)\n",
    "        except ParseException as e:\n",
    "            print(f\"Invalid SGF file: {sgf_path}\")\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "        # Parse the SGF file\n",
    "        with open(sgf_path, \"r\", encoding='ISO-8859-1') as f:\n",
    "            collection = sgf.parse(f.read())\n",
    "\n",
    "        # Extract the desired data from the SGF file\n",
    "        game_tree = collection.children[0]\n",
    "        root_node = game_tree.root\n",
    "        game_info = root_node.properties\n",
    "        moves = game_tree.nodes[1:]  # Exclude the root node\n",
    "\n",
    "        # Initialize variables to track the current game's data\n",
    "        curr_date = game_info.get(\"DT\", \"\")\n",
    "        curr_moves = []\n",
    "        curr_color = None\n",
    "\n",
    "        # Add the current game's moves to the moves list\n",
    "        for move_num, node in enumerate(moves, start=1):\n",
    "            if 'B' in node.properties:\n",
    "                curr_color = 'B'\n",
    "                move = node.properties['B'][0]\n",
    "            elif 'W' in node.properties:\n",
    "                curr_color = 'W'\n",
    "                move = node.properties['W'][0]\n",
    "            else:\n",
    "                continue\n",
    "            move_type = \"P\" if move == \"\" else move[0]\n",
    "            curr_moves.append((f\"{curr_color}{move_num}\", move, move_type, curr_color))\n",
    "\n",
    "        # Write the current game to the CSV file\n",
    "        row = [\n",
    "            game_info.get(\"SZ\", \"\"),\n",
    "            game_info.get(\"FF\", \"\"),\n",
    "            game_info.get(\"PW\", \"\"),\n",
    "            game_info.get(\"PB\", \"\"),\n",
    "            game_info.get(\"DT\", \"\"),\n",
    "            game_info.get(\"KM\", \"\"),\n",
    "            game_info.get(\"RE\", \"\"),\n",
    "            \" \".join([f\"{move[0]}{move[1]}\" for move in curr_moves]),\n",
    "            \" \".join([move[3] for move in curr_moves]),\n",
    "            \" \".join([move[2] for move in curr_moves])\n",
    "        ]\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"Conversion completed successfully.\")\n",
    "\n",
    "#Run Time: ~4 min "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(output_csv_file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Pysaprk to make a Spark session as it can handle bigger data sets more efficiently and reposits data faster than pandas will. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, unix_timestamp, expr\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import round\n",
    "spark = SparkSession.builder.appName('RUN1').config(\"spark.sql.caseSensitive\", \"True\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(output_csv_file, header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Selecting the Result of Game column and show its distinct values that don't have B or W in them\n",
    "distinct_results = df.select(col(\"Result of Game\")).where(~col(\"Result of Game\").like(\"%B%\")).where(~col(\"Result of Game\").like(\"%W%\")).distinct()\n",
    "distinct_results.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to organize the data...\n",
    ">\n",
    "- Added a new column labeled 'Game Result'\n",
    "\n",
    "- Created different labeles according to the 'Result of Game' column: \n",
    "    1. +F: Win by forfeit\n",
    "\n",
    "    2. +R: Win by resignation\n",
    "    \n",
    "    3. +T: Win by time exceeding \n",
    "    \n",
    "    4. +Jigo: Win by zero (Jigo)\n",
    "    \n",
    "    5. B+: Win\n",
    "    \n",
    "    6. W+: Win\n",
    "    \n",
    "    7. Void: Unfinished\n",
    "    \n",
    "    8. Left unifished: Unifinshed \n",
    "    \n",
    "    9. ?: Unfinished\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import when, col, split, array_contains\n",
    "\n",
    "df_with_result = df.withColumn(\n",
    "    \"Game Result\",\n",
    "    # First, handle the specific cases of 'B+F' and 'W+F' as 'Win by forfeit'\n",
    "    when(col(\"Result of Game\").rlike(r\"(B|W)\\+F\"), \"Win by forfeit\")\n",
    "    .when(col(\"Result of Game\").contains(\"+R\"), \"Win by resignation\")\n",
    "    .when(col(\"Result of Game\").contains(\"+T\"), \"Win by time exceeding\")\n",
    "    .when(col(\"Result of Game\").contains(\"+Jigo\"), \"Win by zero (Jigo)\")\n",
    "    # Next, handle general 'Win' cases (matching 'W+' or 'B+')\n",
    "    .when(col(\"Result of Game\").rlike(r\"(W|B)\\+.*\"), \"Win\")\n",
    "    # Now, handle cases with '?', set to None (Null)\n",
    "    .when(array_contains(split(col(\"Result of Game\"), \",\"), \"?\"), None)\n",
    "    # Next, handle the specific case of 'Jigo' as 'Win by zero (Jigo)'\n",
    "    .when(col(\"Result of Game\").like(\"%Jigo%\"), \"Win by zero (Jigo)\")\n",
    "    # Now, handle cases starting with '?', set to None (Null)\n",
    "    .when(col(\"Result of Game\").startswith(\"?\"), None)\n",
    "    # Handle other specific cases ('Void' and 'Left unfinished')\n",
    "    .when(col(\"Result of Game\") == \"Void\", \"Void\")\n",
    "    .when(col(\"Result of Game\") == \"Left unfinished\", \"Unfinished\")\n",
    "    # Finally, handle all other cases as 'Unfinished'\n",
    "    .otherwise(\"Unfinished\")\n",
    ").filter(~array_contains(split(col(\"Result of Game\"), \",\"), \"?\"))\n",
    "\n",
    "df_with_result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the Margin of Victory column \n",
    "\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "#Adding a new column to the DataFrame with the margin of victory\n",
    "df_with_result_and_margin = df_with_result.withColumn(\n",
    "    \"Margin of Victory\",\n",
    "    regexp_extract(col(\"Result of Game\"), r\"\\+(\\d+(\\.\\d+)?)\", 1)\n",
    ")\n",
    "\n",
    "#Showing the contents of the DataFrame\n",
    "df_with_result_and_margin.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only showing unfinished games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering the DataFrame to show only rows with Unfinished game results\n",
    "unfinished_df = df_with_result_and_margin.filter(col(\"Game Result\").isin(\"Unfinished\"))\n",
    "\n",
    "#Showing the contents of the filtered DataFrame\n",
    "unfinished_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only showing Jigo games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering the DataFrame to show only rows with Jigo game results\n",
    "jigo_df = df_with_result_and_margin.filter(col(\"Game Result\").isin(\"Win by zero (Jigo)\"))\n",
    "\n",
    "#Showing the contents of the filtered DataFrame\n",
    "jigo_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only showing win by time exceeding games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering the DataFrame to show only rows with win by exceeding time game results\n",
    "win_by_time_df = df_with_result_and_margin.filter(col(\"Game Result\").isin(\"Win by time exceeding\"))\n",
    "\n",
    "#Showing the contents of the filtered DataFrame\n",
    "win_by_time_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only showing win games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering the DataFrame to show only rows with Win game results\n",
    "win_df = df_with_result_and_margin.filter(col(\"Game Result\").isin(\"Win\"))\n",
    "\n",
    "#Showing the contents of the filtered DataFrame\n",
    "win_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only showing forfeit games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering the DataFrame to show only rows with Win game results\n",
    "forfeit_df = df_with_result_and_margin.filter(col(\"Game Result\").isin(\"Win by forfeit\"))\n",
    "\n",
    "#Showing the contents of the filtered DataFrame\n",
    "forfeit_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Cleaned Data to a New CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "# Specify the output path for the new CSV file\n",
    "output_path = \"Clean_Go_Data.csv\"\n",
    "\n",
    "# Coalesce the DataFrame to a single partition (so it can all go into 1 file)\n",
    "df_single_partition = df_with_result_and_margin.coalesce(1)\n",
    "\n",
    "# Save the single partition DataFrame as a CSV file\n",
    "df_single_partition.write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "# Check if the file was created\n",
    "if os.path.exists(output_path):\n",
    "    print(\"CSV file was created.\")\n",
    "else:\n",
    "    print(\"CSV file was NOT created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Done"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
